{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8-F70Zu5EGC"
      },
      "source": [
        "##Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wF1_BEo0GtQ5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NQg8UO5GnFPg"
      },
      "outputs": [],
      "source": [
        "!pip install pytextrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7wc73O8knNQH"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWUr6Uc4OVkr"
      },
      "outputs": [],
      "source": [
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONuGqasSvsDm"
      },
      "outputs": [],
      "source": [
        "!pip install kaleido\n",
        "!pip install -U kaleido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5balq7DO5Y8B"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDlB_zbmNyY8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import spacy\n",
        "import pytextrank\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvWMYz5GIAxX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qC_3y_kIDMs"
      },
      "outputs": [],
      "source": [
        "df_real = pd.read_csv('/content/drive/MyDrive/DatasetForColab/Zeroshot1/true.csv')\n",
        "df_fake = pd.read_csv('/content/drive/MyDrive/DatasetForColab/Zeroshot1/fake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ94YrtOHUwH"
      },
      "outputs": [],
      "source": [
        "df_real.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJHIw80u5VGR"
      },
      "outputs": [],
      "source": [
        "df_fake.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54167-_c76YH"
      },
      "source": [
        "## Combine Both Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbPC8p7f6L7x"
      },
      "outputs": [],
      "source": [
        "df_real['label'] = 'real'\n",
        "df_fake['label'] = 'fake'\n",
        "\n",
        "# Step 3: Combine both DataFrames\n",
        "df = pd.concat([df_real, df_fake], ignore_index=True)\n",
        "\n",
        "# Step 4: Shuffle the data\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Optional: Check balance\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu6g9_PZ6o34"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_Mfp7Pw_40W"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9WGZM-gA2mF"
      },
      "source": [
        "##Summarize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg1DsaH4A67l"
      },
      "outputs": [],
      "source": [
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp.add_pipe(\"textrank\")\n",
        "\n",
        "def summarize_text(text, limit_phrases=2, limit_sentences=2):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        summary_sents = [sent.text for sent in doc._.textrank.summary(limit_phrases=limit_phrases, limit_sentences=limit_sentences)]\n",
        "        summary = ' '.join(summary_sents)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing text: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "save_path = '/content/drive/MyDrive/DatasetForColab/Zeroshot1/df_with_summary.json'\n",
        "\n",
        "# Load existing results if available, else create empty DataFrame\n",
        "if os.path.exists(save_path):\n",
        "    df_results = pd.read_json(save_path, lines=True)\n",
        "    start_index = len(df_results)\n",
        "    print(f\"Resuming from row {start_index}\")\n",
        "else:\n",
        "    df_results = pd.DataFrame(columns=list(df.columns) + ['summary'])\n",
        "    start_index = 0\n",
        "\n",
        "for i in tqdm(range(start_index, len(df))):\n",
        "    row = df.iloc[i]\n",
        "    summary = summarize_text(row['text'])\n",
        "    new_row = row.to_dict()\n",
        "    new_row['summary'] = summary\n",
        "\n",
        "    df_results = pd.concat([df_results, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    if (i + 1) % 2000 == 0 or (i + 1) == len(df):\n",
        "        df_results.to_json(save_path, orient='records', lines=True)\n",
        "        print(f\"Saved progress at row {i + 1}\")\n",
        "\n",
        "print(\"Summarization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pcSKVs9FYsr"
      },
      "outputs": [],
      "source": [
        "df_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdyD92KY8NM3"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md2uxTH58Tax"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Preprocessing ---\n",
        "def preprocess_text_dataset(df, text_column='text', new_column='clean_text'):\n",
        "    def clean_text(text):\n",
        "        text = str(text) if not pd.isnull(text) else \"\"\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove punctuation/special chars\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    df[new_column] = df[text_column].apply(clean_text)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM2GDCH7FJo-"
      },
      "outputs": [],
      "source": [
        "df = preprocess_text_dataset(df_results, text_column='summary', new_column='clean_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hangw16CJk8t"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE4Gu1xZugVV"
      },
      "outputs": [],
      "source": [
        "df.head()\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjS_8kyu5dVt"
      },
      "source": [
        "## Removing empty rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfLFoHk6_4Nc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check label distribution before removing\n",
        "print(\"Before cleaning:\")\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# -------------------------------\n",
        "# Remove rows with ANY empty column\n",
        "# -------------------------------\n",
        "# Step 1: Replace empty strings with NaN\n",
        "df.replace(\"\", pd.NA, inplace=True)\n",
        "\n",
        "# Step 2: Drop rows with any NaN values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Step 3: Reset index after dropping\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Final checks\n",
        "# -------------------------------\n",
        "print(\"\\nAfter cleaning:\")\n",
        "print(df[\"label\"].value_counts())\n",
        "print(\"Final shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN3z1iCI8E6A"
      },
      "source": [
        "## Balance the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tHvnayaCcBs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check label distribution before downsampling\n",
        "print(\"Before downsampling:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = df['label'].value_counts().idxmax()\n",
        "minority_class = df['label'].value_counts().idxmin()\n",
        "\n",
        "df_majority = df[df['label'] == majority_class]\n",
        "df_minority = df[df['label'] == minority_class]\n",
        "\n",
        "# Downsample majority class to minority class size\n",
        "df_majority_downsampled = resample(\n",
        "    df_majority,\n",
        "    replace=False,               # sample without replacement\n",
        "    n_samples=len(df_minority),  # match minority size\n",
        "    random_state=42              # reproducible results\n",
        ")\n",
        "\n",
        "# Combine downsampled majority class with minority class\n",
        "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "# Shuffle the balanced dataframe\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Check the new label distribution\n",
        "print(\"\\nAfter downsampling and balancing:\")\n",
        "print(df_balanced['label'].value_counts())\n",
        "print(\"Balanced shape:\", df_balanced.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK9dhF4EKX69"
      },
      "outputs": [],
      "source": [
        "df_balanced.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv9yJpfmF-HT"
      },
      "outputs": [],
      "source": [
        "df_balanced.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8EomFI09G0g"
      },
      "source": [
        "## MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPvPX2GVJuaA"
      },
      "outputs": [],
      "source": [
        "# === Config ===\n",
        "MODEL_NAME = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Deberta_maxscore_results(0.8).json\"\n",
        "THRESHOLD = 0.8\n",
        "LABELS = [\n",
        "    \"This news is real and based on facts.\",\n",
        "    \"This news is fake and contains misinformation.\"\n",
        "]\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "classifier = pipeline(\"zero-shot-classification\", model=MODEL_NAME, device=0)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# === Load your dataset ===\n",
        "df_balanced = df_balanced.copy()\n",
        "texts = df_balanced[\"clean_text\"].tolist()\n",
        "true_labels = df_balanced[\"label\"].tolist()\n",
        "\n",
        "# === Chunking function ===\n",
        "def chunk_text(text, tokenizer, max_tokens=512):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk = tokenizer.convert_tokens_to_string(tokens[i:i + max_tokens])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# === Classifier function ===\n",
        "def classify_with_max_chunk(text, tokenizer, classifier, labels, threshold=0.8):\n",
        "    chunks = chunk_text(text, tokenizer)\n",
        "    score_dict = defaultdict(float)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        result = classifier(chunk, candidate_labels=labels)\n",
        "        for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "            score_dict[label] = max(score_dict[label], score)\n",
        "\n",
        "    final_label = max(score_dict, key=score_dict.get)\n",
        "    final_score = score_dict[final_label]\n",
        "\n",
        "    if final_score >= threshold:\n",
        "        return final_label, final_score\n",
        "    else:\n",
        "        return \"uncertain\", final_score\n",
        "\n",
        "# === Load saved progress ===\n",
        "try:\n",
        "    with open(SAVE_PATH, 'r') as f:\n",
        "        saved_results = json.load(f)\n",
        "    print(f\"‚úÖ Loaded {len(saved_results)} saved results.\")\n",
        "except FileNotFoundError:\n",
        "    saved_results = []\n",
        "    print(\"üÜï No saved results found. Starting fresh.\")\n",
        "\n",
        "start_index = len(saved_results)\n",
        "\n",
        "# === Run classification ===\n",
        "for i in tqdm(range(start_index, len(texts)), desc=\"Classifying\"):\n",
        "    text = texts[i]\n",
        "    true_label = true_labels[i]\n",
        "\n",
        "    predicted_label, score = classify_with_max_chunk(\n",
        "        text, tokenizer, classifier, LABELS, threshold=THRESHOLD\n",
        "    )\n",
        "\n",
        "    saved_results.append({\n",
        "        \"index\": i,\n",
        "        \"text\": text,\n",
        "        \"true_label\": true_label,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"confidence\": round(score, 4)\n",
        "    })\n",
        "  # Save every 2000 rows or at the very end\n",
        "    if (i + 1) % 2000 == 0 or (i + 1) == len(texts):\n",
        "        with open(SAVE_PATH, 'w') as f:\n",
        "            json.dump(saved_results, f, indent=2)\n",
        "        print(f\"üíæ Saved progress at {i + 1} rows.\")\n",
        "\n",
        "print(f\"‚úÖ Classification complete. {len(saved_results)} articles processed and saved to JSON.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5863j8A9xLHe"
      },
      "outputs": [],
      "source": [
        "# Define threshold\n",
        "THRESHOLD = 0.8\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(saved_results)\n",
        "\n",
        "# Map predicted labels to 'real', 'fake', or 'uncertain'\n",
        "label_mapping = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\": \"fake\"\n",
        "}\n",
        "df_results['predicted_label_mapped'] = df_results['predicted_label'].map(label_mapping).fillna('uncertain')\n",
        "\n",
        "# Filter out 'uncertain' predictions\n",
        "df_filtered = df_results[df_results['predicted_label_mapped'] != 'uncertain'].copy()\n",
        "df_filtered['is_correct'] = (df_filtered['true_label'] == df_filtered['predicted_label_mapped'])\n",
        "\n",
        "# Bin confidence scores\n",
        "bins = np.arange(0, 1.01, 0.1)\n",
        "bin_labels = [f'{b:.1f}-{b + 0.1:.1f}' for b in bins[:-1]]  # ‚Üê fixed here\n",
        "df_filtered['confidence_bin'] = pd.cut(df_filtered['confidence'], bins=bins, right=False, labels=bin_labels)\n",
        "\n",
        "\n",
        "# Calculate accuracy per bin (‚ö†Ô∏è fixed warning by adding observed=True)\n",
        "confidence_accuracy = df_filtered.groupby('confidence_bin', observed=True)['is_correct'].mean().reset_index()\n",
        "confidence_accuracy = confidence_accuracy.rename(columns={'is_correct': 'accuracy'})\n",
        "\n",
        "# Count samples in each bin (‚ö†Ô∏è fixed warning by adding observed=True)\n",
        "bin_counts = df_filtered.groupby('confidence_bin', observed=True).size().reset_index(name='count')\n",
        "\n",
        "# Merge data\n",
        "confidence_data = pd.merge(confidence_accuracy, bin_counts, on='confidence_bin')\n",
        "\n",
        "# Calculate cumulative values\n",
        "confidence_data['bin_midpoint'] = confidence_data['confidence_bin'].apply(lambda x: float(x.split('-')[0]) + 0.05 if pd.notna(x) else np.nan)\n",
        "confidence_data = confidence_data.sort_values('bin_midpoint').reset_index(drop=True)\n",
        "confidence_data['cumulative_count'] = confidence_data['count'].cumsum()\n",
        "confidence_data['cumulative_correct'] = (confidence_data['accuracy'] * confidence_data['count']).cumsum()\n",
        "confidence_data['cumulative_accuracy'] = confidence_data['cumulative_correct'] / confidence_data['cumulative_count']\n",
        "\n",
        "# Drop midpoint if not needed\n",
        "confidence_data = confidence_data.drop(columns=['bin_midpoint'])\n",
        "\n",
        "# Drop empty bins\n",
        "confidence_data = confidence_data.dropna(subset=['cumulative_accuracy'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(confidence_data['confidence_bin'], confidence_data['cumulative_accuracy'], marker='o', linestyle='-', label='Cumulative Accuracy')\n",
        "\n",
        "# Add horizontal line for overall accuracy\n",
        "overall_accuracy = df_filtered['is_correct'].mean()\n",
        "plt.axhline(y=overall_accuracy, color='r', linestyle='--', label='Overall Accuracy')\n",
        "\n",
        "# Add vertical line for threshold bin\n",
        "threshold_bin = f'{THRESHOLD:.1f}-{THRESHOLD + 0.1:.1f}'\n",
        "if threshold_bin in confidence_data['confidence_bin'].values:\n",
        "    plt.axvline(x=threshold_bin, color='g', linestyle='--', label=f'Threshold ({THRESHOLD:.1f})')\n",
        "\n",
        "# Format plot\n",
        "plt.xlabel('Confidence Bin')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Confidence Curve(DeBERTa-v3-large)')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confidence_curve(DeBERTa-v3-large).png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Optional: Print the data table\n",
        "print(\"\\nConfidence Data:\")\n",
        "print(confidence_data[['confidence_bin', 'count', 'accuracy', 'cumulative_count', 'cumulative_accuracy']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T11ulL0G5Uq5"
      },
      "outputs": [],
      "source": [
        "# prompt: distribution of confidence scores across all predictions.\n",
        "\n",
        "# Plotting the distribution of confidence scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_results['confidence'], bins=20, kde=True)\n",
        "plt.title('Distribution of Confidence Scores(DeBERTa-v3-large)')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.savefig('confidence_score_distribution(DeBERTa-v3-large).png')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Basic statistics of confidence scores\n",
        "print(\"\\nConfidence Score Statistics:\")\n",
        "print(df_results['confidence'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DziNGHQszMH7"
      },
      "outputs": [],
      "source": [
        "# prompt: confusion matrix for th eabove model\n",
        "\n",
        "# Get true and predicted labels from the filtered DataFrame\n",
        "true_labels_filtered = df_filtered['true_label']\n",
        "predicted_labels_filtered = df_filtered['predicted_label_mapped']\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_labels_filtered, predicted_labels_filtered, labels=['real', 'fake'])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['real', 'fake'], yticklabels=['real', 'fake'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Filtered: Excluding \"uncertain\")')\n",
        "plt.savefig('confusion_matrix_filtered(DeBERTa-v3-large).png')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Calculate and print confusion matrix values\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rCGS5gGBEqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23c1CWO4zcVy"
      },
      "source": [
        "##facebook/bart-large-mnli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW0cHoLtzm-K"
      },
      "outputs": [],
      "source": [
        "# === Config ===\n",
        "MODEL_NAME = \"facebook/bart-large-mnli\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/bart_maxscore_results(0.8).json\"\n",
        "THRESHOLD = 0.8\n",
        "LABELS = [\n",
        "    \"This news is real and based on facts.\",\n",
        "    \"This news is fake and contains misinformation.\"\n",
        "]\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "classifier = pipeline(\"zero-shot-classification\", model=MODEL_NAME, device=0)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# === Load your dataset ===\n",
        "df_balanced = df_balanced.copy()\n",
        "texts = df_balanced[\"clean_text\"].tolist()\n",
        "true_labels = df_balanced[\"label\"].tolist()\n",
        "\n",
        "# === Chunking function ===\n",
        "def chunk_text(text, tokenizer, max_tokens=512):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk = tokenizer.convert_tokens_to_string(tokens[i:i + max_tokens])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# === Classifier function ===\n",
        "def classify_with_max_chunk(text, tokenizer, classifier, labels, threshold=0.8):\n",
        "    chunks = chunk_text(text, tokenizer)\n",
        "    score_dict = defaultdict(float)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        result = classifier(chunk, candidate_labels=labels)\n",
        "        for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "            score_dict[label] = max(score_dict[label], score)\n",
        "\n",
        "    final_label = max(score_dict, key=score_dict.get)\n",
        "    final_score = score_dict[final_label]\n",
        "\n",
        "    if final_score >= threshold:\n",
        "        return final_label, final_score\n",
        "    else:\n",
        "        return \"uncertain\", final_score\n",
        "\n",
        "# === Load saved progress ===\n",
        "try:\n",
        "    with open(SAVE_PATH, 'r') as f:\n",
        "        saved_results = json.load(f)\n",
        "    print(f\"‚úÖ Loaded {len(saved_results)} saved results.\")\n",
        "except FileNotFoundError:\n",
        "    saved_results = []\n",
        "    print(\"üÜï No saved results found. Starting fresh.\")\n",
        "\n",
        "start_index = len(saved_results)\n",
        "\n",
        "# === Run classification ===\n",
        "for i in tqdm(range(start_index, len(texts)), desc=\"Classifying\"):\n",
        "    text = texts[i]\n",
        "    true_label = true_labels[i]\n",
        "\n",
        "    predicted_label, score = classify_with_max_chunk(\n",
        "        text, tokenizer, classifier, LABELS, threshold=THRESHOLD\n",
        "    )\n",
        "\n",
        "    saved_results.append({\n",
        "        \"index\": i,\n",
        "        \"text\": text,\n",
        "        \"true_label\": true_label,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"confidence\": round(score, 4)\n",
        "    })\n",
        "  # Save every 2000 rows or at the very end\n",
        "    if (i + 1) % 2000 == 0 or (i + 1) == len(texts):\n",
        "        with open(SAVE_PATH, 'w') as f:\n",
        "            json.dump(saved_results, f, indent=2)\n",
        "        print(f\"üíæ Saved progress at {i + 1} rows.\")\n",
        "\n",
        "print(f\"‚úÖ Classification complete. {len(saved_results)} articles processed and saved to JSON.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnOaGqAgNGzX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Config ===\n",
        "SAVE_PATH = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/bart_maxscore_results(0.8).json\"\n",
        "THRESHOLD = 0.8\n",
        "\n",
        "# === Load JSON (array format) ===\n",
        "with open(SAVE_PATH, \"r\") as f:\n",
        "    saved_results = json.load(f)\n",
        "\n",
        "# === Convert to DataFrame ===\n",
        "df_results = pd.DataFrame(saved_results)\n",
        "\n",
        "# === Label Mapping ===\n",
        "label_mapping = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\": \"fake\"\n",
        "}\n",
        "df_results['predicted_label_mapped'] = df_results['predicted_label'].map(label_mapping).fillna('uncertain')\n",
        "\n",
        "# === Filter out uncertain predictions ===\n",
        "df_filtered = df_results[df_results['predicted_label_mapped'] != 'uncertain'].copy()\n",
        "df_filtered['is_correct'] = (df_filtered['true_label'] == df_filtered['predicted_label_mapped'])\n",
        "\n",
        "# === Bin confidence scores ===\n",
        "bins = np.arange(0, 1.01, 0.1)\n",
        "bin_labels = [f'{b:.1f}-{b + 0.1:.1f}' for b in bins[:-1]]\n",
        "df_filtered['confidence_bin'] = pd.cut(df_filtered['confidence'], bins=bins, right=False, labels=bin_labels)\n",
        "\n",
        "# === Accuracy per bin ===\n",
        "confidence_accuracy = df_filtered.groupby('confidence_bin', observed=True)['is_correct'].mean().reset_index()\n",
        "confidence_accuracy = confidence_accuracy.rename(columns={'is_correct': 'accuracy'})\n",
        "\n",
        "# === Count per bin ===\n",
        "bin_counts = df_filtered.groupby('confidence_bin', observed=True).size().reset_index(name='count')\n",
        "\n",
        "# === Merge accuracy + counts ===\n",
        "confidence_data = pd.merge(confidence_accuracy, bin_counts, on='confidence_bin')\n",
        "\n",
        "# === Cumulative calculations ===\n",
        "confidence_data['bin_midpoint'] = confidence_data['confidence_bin'].apply(\n",
        "    lambda x: float(x.split('-')[0]) + 0.05 if pd.notna(x) else np.nan\n",
        ")\n",
        "confidence_data = confidence_data.sort_values('bin_midpoint').reset_index(drop=True)\n",
        "confidence_data['cumulative_count'] = confidence_data['count'].cumsum()\n",
        "confidence_data['cumulative_correct'] = (confidence_data['accuracy'] * confidence_data['count']).cumsum()\n",
        "confidence_data['cumulative_accuracy'] = confidence_data['cumulative_correct'] / confidence_data['cumulative_count']\n",
        "\n",
        "# === Clean up and drop bins with no accuracy ===\n",
        "confidence_data = confidence_data.drop(columns=['bin_midpoint'])\n",
        "confidence_data = confidence_data.dropna(subset=['cumulative_accuracy'])\n",
        "\n",
        "# === Plot ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(confidence_data['confidence_bin'], confidence_data['cumulative_accuracy'],\n",
        "         marker='o', linestyle='-', label='Cumulative Accuracy')\n",
        "\n",
        "# Horizontal line for overall accuracy\n",
        "overall_accuracy = df_filtered['is_correct'].mean()\n",
        "plt.axhline(y=overall_accuracy, color='r', linestyle='--', label='Overall Accuracy')\n",
        "\n",
        "# Vertical line for threshold\n",
        "threshold_bin = f'{THRESHOLD:.1f}-{THRESHOLD + 0.1:.1f}'\n",
        "if threshold_bin in confidence_data['confidence_bin'].values:\n",
        "    plt.axvline(x=threshold_bin, color='g', linestyle='--', label=f'Threshold ({THRESHOLD:.1f})')\n",
        "\n",
        "# === Plot formatting ===\n",
        "plt.xlabel('Confidence Bin')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Confidence Curve (facebook/bart-large-mnli)')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confidence_curve_bart.png')\n",
        "plt.show()\n",
        "\n",
        "# === Print table ===\n",
        "print(\"\\nConfidence Data:\")\n",
        "print(confidence_data[['confidence_bin', 'count', 'accuracy', 'cumulative_count', 'cumulative_accuracy']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpqGRW9n6Fgh"
      },
      "outputs": [],
      "source": [
        "# prompt: distribusion of confidence score for bart mdel\n",
        "\n",
        "# Plotting the distribution of confidence scores for the BART model\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(pd.DataFrame(saved_results)['confidence'], bins=20, kde=True)\n",
        "plt.title('Distribution of Confidence Scores (BART)')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.savefig('confidence_score_distribution_bart.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_MLejaLz3_C"
      },
      "outputs": [],
      "source": [
        "# prompt: only confusion matrix\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results_bart = pd.DataFrame(saved_results)\n",
        "\n",
        "# Map predicted labels to 'real', 'fake', or 'uncertain'\n",
        "label_mapping = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\"\n",
        ": \"fake\"\n",
        "}\n",
        "df_results_bart['predicted_label_mapped'] = df_results_bart['predicted_label'].map(label_mapping).fillna('uncertain')\n",
        "\n",
        "# Filter out 'uncertain' predictions\n",
        "df_filtered_bart = df_results_bart[df_results_bart['predicted_label_mapped'] != 'uncertain'].copy()\n",
        "df_filtered_bart['is_correct'] = (df_filtered_bart['true_label'] == df_filtered_bart['predicted_label_mapped'])\n",
        "\n",
        "# Get true and predicted labels from the filtered DataFrame for BART\n",
        "true_labels_filtered_bart = df_filtered_bart['true_label']\n",
        "predicted_labels_filtered_bart = df_filtered_bart['predicted_label_mapped']\n",
        "\n",
        "# Generate the confusion matrix for BART\n",
        "cm_bart = confusion_matrix(true_labels_filtered_bart, predicted_labels_filtered_bart, labels=['real', 'fake'])\n",
        "\n",
        "# Plot the confusion matrix for BART\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_bart, annot=True, fmt='d', cmap='Blues', xticklabels=['real', 'fake'], yticklabels=['real', 'fake'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (BART - Filtered: Excluding \"uncertain\")')\n",
        "plt.savefig('confusion_matrix_filtered_bart.png')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Calculate and print confusion matrix values for BART\n",
        "print(\"\\nConfusion Matrix (BART):\")\n",
        "cm_bart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ExHDnJB0Bg6"
      },
      "source": [
        "## MoritzLaurer/ModernBERT-large-zeroshot-v2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT1ju2lw0Cgq"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('high')  # Enables TF32-optimized matmul\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CllMoZLs0OBa"
      },
      "outputs": [],
      "source": [
        "# === Config ===\n",
        "MODEL_NAME = \"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Modernbert_maxscore_results(0.8).json\"\n",
        "THRESHOLD = 0.8\n",
        "LABELS = [\n",
        "    \"This news is real and based on facts.\",\n",
        "    \"This news is fake and contains misinformation.\"\n",
        "]\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "classifier = pipeline(\"zero-shot-classification\", model=MODEL_NAME, device=0)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# === Load your dataset ===\n",
        "df_balanced = df_balanced.copy()\n",
        "texts = df_balanced[\"clean_text\"].tolist()\n",
        "true_labels = df_balanced[\"label\"].tolist()\n",
        "\n",
        "# === Chunking function ===\n",
        "def chunk_text(text, tokenizer, max_tokens=512):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk = tokenizer.convert_tokens_to_string(tokens[i:i + max_tokens])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# === Classifier function ===\n",
        "def classify_with_max_chunk(text, tokenizer, classifier, labels, threshold=0.8):\n",
        "    chunks = chunk_text(text, tokenizer)\n",
        "    score_dict = defaultdict(float)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        result = classifier(chunk, candidate_labels=labels)\n",
        "        for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "            score_dict[label] = max(score_dict[label], score)\n",
        "\n",
        "    final_label = max(score_dict, key=score_dict.get)\n",
        "    final_score = score_dict[final_label]\n",
        "\n",
        "    if final_score >= threshold:\n",
        "        return final_label, final_score\n",
        "    else:\n",
        "        return \"uncertain\", final_score\n",
        "\n",
        "# === Load saved progress ===\n",
        "try:\n",
        "    with open(SAVE_PATH, 'r') as f:\n",
        "        saved_results = json.load(f)\n",
        "    print(f\"‚úÖ Loaded {len(saved_results)} saved results.\")\n",
        "except FileNotFoundError:\n",
        "    saved_results = []\n",
        "    print(\"üÜï No saved results found. Starting fresh.\")\n",
        "\n",
        "start_index = len(saved_results)\n",
        "\n",
        "# === Run classification ===\n",
        "for i in tqdm(range(start_index, len(texts)), desc=\"Classifying\"):\n",
        "    text = texts[i]\n",
        "    true_label = true_labels[i]\n",
        "\n",
        "    predicted_label, score = classify_with_max_chunk(\n",
        "        text, tokenizer, classifier, LABELS, threshold=THRESHOLD\n",
        "    )\n",
        "\n",
        "    saved_results.append({\n",
        "        \"index\": i,\n",
        "        \"text\": text,\n",
        "        \"true_label\": true_label,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"confidence\": round(score, 4)\n",
        "    })\n",
        "  # Save every 2000 rows or at the very end\n",
        "    if (i + 1) % 2000 == 0 or (i + 1) == len(texts):\n",
        "        with open(SAVE_PATH, 'w') as f:\n",
        "            json.dump(saved_results, f, indent=2)\n",
        "        print(f\"üíæ Saved progress at {i + 1} rows.\")\n",
        "\n",
        "print(f\"‚úÖ Classification complete. {len(saved_results)} articles processed and saved to JSON.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMu9ydZ2N4oD"
      },
      "outputs": [],
      "source": [
        "# prompt: confidence curve for  modernbert\n",
        "\n",
        "# === Config for ModernBERT ===\n",
        "SAVE_PATH_MODERNBERT = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Modernbert_maxscore_results(0.8).json\"\n",
        "THRESHOLD_MODERNBERT = 0.8\n",
        "\n",
        "# === Load JSON (array format) for ModernBERT ===\n",
        "with open(SAVE_PATH_MODERNBERT, \"r\") as f:\n",
        "    saved_results_modernbert = json.load(f)\n",
        "\n",
        "# === Convert to DataFrame for ModernBERT ===\n",
        "df_results_modernbert = pd.DataFrame(saved_results_modernbert)\n",
        "\n",
        "# === Label Mapping for ModernBERT ===\n",
        "label_mapping_modernbert = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\": \"fake\"\n",
        "}\n",
        "df_results_modernbert['predicted_label_mapped'] = df_results_modernbert['predicted_label'].map(label_mapping_modernbert).fillna('uncertain')\n",
        "\n",
        "# === Filter out uncertain predictions for ModernBERT ===\n",
        "df_filtered_modernbert = df_results_modernbert[df_results_modernbert['predicted_label_mapped'] != 'uncertain'].copy()\n",
        "df_filtered_modernbert['is_correct'] = (df_filtered_modernbert['true_label'] == df_filtered_modernbert['predicted_label_mapped'])\n",
        "\n",
        "# === Bin confidence scores for ModernBERT ===\n",
        "bins = np.arange(0, 1.01, 0.1)\n",
        "bin_labels = [f'{b:.1f}-{b + 0.1:.1f}' for b in bins[:-1]]\n",
        "df_filtered_modernbert['confidence_bin'] = pd.cut(df_filtered_modernbert['confidence'], bins=bins, right=False, labels=bin_labels)\n",
        "\n",
        "# === Accuracy per bin for ModernBERT ===\n",
        "confidence_accuracy_modernbert = df_filtered_modernbert.groupby('confidence_bin', observed=True)['is_correct'].mean().reset_index()\n",
        "confidence_accuracy_modernbert = confidence_accuracy_modernbert.rename(columns={'is_correct': 'accuracy'})\n",
        "\n",
        "# === Count per bin for ModernBERT ===\n",
        "bin_counts_modernbert = df_filtered_modernbert.groupby('confidence_bin', observed=True).size().reset_index(name='count')\n",
        "\n",
        "# === Merge accuracy + counts for ModernBERT ===\n",
        "confidence_data_modernbert = pd.merge(confidence_accuracy_modernbert, bin_counts_modernbert, on='confidence_bin')\n",
        "\n",
        "# === Cumulative calculations for ModernBERT ===\n",
        "confidence_data_modernbert['bin_midpoint'] = confidence_data_modernbert['confidence_bin'].apply(\n",
        "    lambda x: float(x.split('-')[0]) + 0.05 if pd.notna(x) else np.nan\n",
        ")\n",
        "confidence_data_modernbert = confidence_data_modernbert.sort_values('bin_midpoint').reset_index(drop=True)\n",
        "confidence_data_modernbert['cumulative_count'] = confidence_data_modernbert['count'].cumsum()\n",
        "confidence_data_modernbert['cumulative_correct'] = (confidence_data_modernbert['accuracy'] * confidence_data_modernbert['count']).cumsum()\n",
        "confidence_data_modernbert['cumulative_accuracy'] = confidence_data_modernbert['cumulative_correct'] / confidence_data_modernbert['cumulative_count']\n",
        "\n",
        "# === Clean up and drop bins with no accuracy for ModernBERT ===\n",
        "confidence_data_modernbert = confidence_data_modernbert.drop(columns=['bin_midpoint'])\n",
        "confidence_data_modernbert = confidence_data_modernbert.dropna(subset=['cumulative_accuracy'])\n",
        "\n",
        "# === Plot Confidence Curve for ModernBERT ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(confidence_data_modernbert['confidence_bin'], confidence_data_modernbert['cumulative_accuracy'],\n",
        "         marker='o', linestyle='-', label='Cumulative Accuracy (ModernBERT)')\n",
        "\n",
        "# Horizontal line for overall accuracy for ModernBERT\n",
        "overall_accuracy_modernbert = df_filtered_modernbert['is_correct'].mean()\n",
        "plt.axhline(y=overall_accuracy_modernbert, color='purple', linestyle='--', label='Overall Accuracy (ModernBERT)')\n",
        "\n",
        "# Vertical line for threshold for ModernBERT\n",
        "threshold_bin_modernbert = f'{THRESHOLD_MODERNBERT:.1f}-{THRESHOLD_MODERNBERT + 0.1:.1f}'\n",
        "if threshold_bin_modernbert in confidence_data_modernbert['confidence_bin'].values:\n",
        "    plt.axvline(x=threshold_bin_modernbert, color='orange', linestyle='--', label=f'Threshold ({THRESHOLD_MODERNBERT:.1f})')\n",
        "\n",
        "\n",
        "# === Plot formatting ===\n",
        "plt.xlabel('Confidence Bin')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Confidence Curve (ModernBERT)')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confidence_curve_modernbert.png')\n",
        "plt.show()\n",
        "\n",
        "# === Print table ===\n",
        "print(\"\\nConfidence Data (ModernBERT):\")\n",
        "print(confidence_data_modernbert[['confidence_bin', 'count', 'accuracy', 'cumulative_count', 'cumulative_accuracy']])\n",
        "\n",
        "# Plotting the distribution of confidence scores for the ModernBERT model\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_results_modernbert['confidence'], bins=20, kde=True)\n",
        "plt.title('Distribution of Confidence Scores (ModernBERT)')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.savefig('confidence_score_distribution_modernbert.png')\n",
        "plt.show()\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results_modernbert = pd.DataFrame(saved_results_modernbert)\n",
        "\n",
        "# Map predicted labels to 'real', 'fake', or 'uncertain'\n",
        "label_mapping = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\": \"fake\"\n",
        "}\n",
        "df_results_modernbert['predicted_label_mapped'] = df_results_modernbert['predicted_label'].map(label_mapping).fillna('uncertain')\n",
        "\n",
        "# Filter out 'uncertain' predictions\n",
        "df_filtered_modernbert = df_results_modernbert[df_results_modernbert['predicted_label_mapped'] != 'uncertain'].copy()\n",
        "df_filtered_modernbert['is_correct'] = (df_filtered_modernbert['true_label'] == df_filtered_modernbert['predicted_label_mapped'])\n",
        "\n",
        "\n",
        "# Get true and predicted labels from the filtered DataFrame for ModernBERT\n",
        "true_labels_filtered_modernbert = df_filtered_modernbert['true_label']\n",
        "predicted_labels_filtered_modernbert = df_filtered_modernbert['predicted_label_mapped']\n",
        "\n",
        "# Generate the confusion matrix for ModernBERT\n",
        "cm_modernbert = confusion_matrix(true_labels_filtered_modernbert, predicted_labels_filtered_modernbert, labels=['real', 'fake'])\n",
        "\n",
        "# Plot the confusion matrix for ModernBERT\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_modernbert, annot=True, fmt='d', cmap='Blues', xticklabels=['real', 'fake'], yticklabels=['real', 'fake'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (ModernBERT - Filtered: Excluding \"uncertain\")')\n",
        "plt.savefig('confusion_matrix_filtered_modernbert.png')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Calculate and print confusion matrix values for ModernBERT\n",
        "print(\"\\nConfusion Matrix (ModernBERT):\")\n",
        "cm_modernbert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z6FOgEP0UGC"
      },
      "outputs": [],
      "source": [
        "# Filter out uncertain predictions\n",
        "filtered_results = [\n",
        "    result for result in saved_results if result[\"predicted_label\"] != \"uncertain\"\n",
        "]\n",
        "\n",
        "# Extract true and predicted labels for evaluation\n",
        "true_labels_eval = [result[\"true_label\"] for result in filtered_results]\n",
        "# Map predicted labels from sentences back to 'real'/'fake'\n",
        "predicted_labels_eval = [\n",
        "    'real' if result[\"predicted_label\"] == LABELS[0] else 'fake'\n",
        "    for result in filtered_results\n",
        "]\n",
        "\n",
        "# Define labels for the confusion matrix\n",
        "labels = ['real', 'fake']\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(true_labels_eval, predicted_labels_eval, labels=labels)\n",
        "\n",
        "# Display the Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix for ModernBERT Zero-shot Classification (Excluding Uncertain)\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate Accuracy on filtered results\n",
        "if filtered_results:\n",
        "    accuracy = accuracy_score(true_labels_eval, predicted_labels_eval)\n",
        "    print(f\"‚úÖ Accuracy (excluding uncertain): {accuracy:.4f}\")\n",
        "else:\n",
        "    print(\"‚ùå No predictions available after filtering uncertain results.\")\n",
        "\n",
        "# Optional: Calculate and print other metrics (Precision, Recall, F1)\n",
        "# precision = precision_score(true_labels_eval, predicted_labels_eval, pos_label='real')\n",
        "# recall = recall_score(true_labels_eval, predicted_labels_eval, pos_label='real')\n",
        "# f1 = f1_score(true_labels_eval, predicted_labels_eval, pos_label='real')\n",
        "# print(f\"Precision (excluding uncertain): {precision:.4f}\")\n",
        "# print(f\"Recall (excluding uncertain): {recall:.4f}\")\n",
        "# print(f\"F1 Score (excluding uncertain): {f1:.4f}\")\n",
        "\n",
        "# Report the number of uncertain predictions\n",
        "num_uncertain = len([\n",
        "    result for result in saved_results if result[\"predicted_label\"] == \"uncertain\"\n",
        "])\n",
        "print(f\"‚ÑπÔ∏è Number of uncertain predictions: {num_uncertain}\")\n",
        "print(f\"Total predictions: {len(saved_results)}\")\n",
        "print(f\"Predictions used for evaluation: {len(filtered_results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYk6p8Kc0iEj"
      },
      "source": [
        "##cross-encoder/nli-deberta-v3-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssHv1sX80ht7"
      },
      "outputs": [],
      "source": [
        "# === Config ===\n",
        "MODEL_NAME = \"cross-encoder/nli-deberta-v3-base\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results(0.8).json\"\n",
        "THRESHOLD = 0.8\n",
        "LABELS = [\n",
        "    \"This news is real and based on facts.\",\n",
        "    \"This news is fake and contains misinformation.\"\n",
        "]\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "classifier = pipeline(\"zero-shot-classification\", model=MODEL_NAME, device=0)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# === Load your dataset ===\n",
        "df_balanced = df_balanced.copy()\n",
        "texts = df_balanced[\"clean_text\"].tolist()\n",
        "true_labels = df_balanced[\"label\"].tolist()\n",
        "\n",
        "# === Chunking function ===\n",
        "def chunk_text(text, tokenizer, max_tokens=512):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk = tokenizer.convert_tokens_to_string(tokens[i:i + max_tokens])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# === Classifier function ===\n",
        "def classify_with_max_chunk(text, tokenizer, classifier, labels, threshold=0.8):\n",
        "    chunks = chunk_text(text, tokenizer)\n",
        "    score_dict = defaultdict(float)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        result = classifier(chunk, candidate_labels=labels)\n",
        "        for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "            score_dict[label] = max(score_dict[label], score)\n",
        "\n",
        "    final_label = max(score_dict, key=score_dict.get)\n",
        "    final_score = score_dict[final_label]\n",
        "\n",
        "    if final_score >= threshold:\n",
        "        return final_label, final_score\n",
        "    else:\n",
        "        return \"uncertain\", final_score\n",
        "\n",
        "# === Load saved progress ===\n",
        "try:\n",
        "    with open(SAVE_PATH, 'r') as f:\n",
        "        saved_results = json.load(f)\n",
        "    print(f\"‚úÖ Loaded {len(saved_results)} saved results.\")\n",
        "except FileNotFoundError:\n",
        "    saved_results = []\n",
        "    print(\"üÜï No saved results found. Starting fresh.\")\n",
        "\n",
        "start_index = len(saved_results)\n",
        "\n",
        "# === Run classification ===\n",
        "for i in tqdm(range(start_index, len(texts)), desc=\"Classifying\"):\n",
        "    text = texts[i]\n",
        "    true_label = true_labels[i]\n",
        "\n",
        "    predicted_label, score = classify_with_max_chunk(\n",
        "        text, tokenizer, classifier, LABELS, threshold=THRESHOLD\n",
        "    )\n",
        "\n",
        "    saved_results.append({\n",
        "        \"index\": i,\n",
        "        \"text\": text,\n",
        "        \"true_label\": true_label,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"confidence\": round(score, 4)\n",
        "    })\n",
        "  # Save every 2000 rows or at the very end\n",
        "    if (i + 1) % 2000 == 0 or (i + 1) == len(texts):\n",
        "        with open(SAVE_PATH, 'w') as f:\n",
        "            json.dump(saved_results, f, indent=2)\n",
        "        print(f\"üíæ Saved progress at {i + 1} rows.\")\n",
        "\n",
        "print(f\"‚úÖ Classification complete. {len(saved_results)} articles processed and saved to JSON.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tshvGerIOdoF"
      },
      "outputs": [],
      "source": [
        "# prompt: confidence curve for cross encoder\n",
        "\n",
        "# === Config for Cross-Encoder DeBERTa ===\n",
        "SAVE_PATH_CE_DEBERTA = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results(0.8).json\"\n",
        "THRESHOLD_CE_DEBERTA = 0.8\n",
        "\n",
        "# === Load JSON (array format) for Cross-Encoder DeBERTa ===\n",
        "with open(SAVE_PATH_CE_DEBERTA, \"r\") as f:\n",
        "    saved_results_ce_deberta = json.load(f)\n",
        "\n",
        "# === Convert to DataFrame for Cross-Encoder DeBERTa ===\n",
        "df_results_ce_deberta = pd.DataFrame(saved_results_ce_deberta)\n",
        "\n",
        "# === Label Mapping for Cross-Encoder DeBERTa ===\n",
        "label_mapping_ce_deberta = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\": \"fake\"\n",
        "}\n",
        "df_results_ce_deberta['predicted_label_mapped'] = df_results_ce_deberta['predicted_label'].map(label_mapping_ce_deberta).fillna('uncertain')\n",
        "\n",
        "# === Filter out uncertain predictions for Cross-Encoder DeBERTa ===\n",
        "df_filtered_ce_deberta = df_results_ce_deberta[df_results_ce_deberta['predicted_label_mapped'] != 'uncertain'].copy()\n",
        "df_filtered_ce_deberta['is_correct'] = (df_filtered_ce_deberta['true_label'] == df_filtered_ce_deberta['predicted_label_mapped'])\n",
        "\n",
        "# === Bin confidence scores for Cross-Encoder DeBERTa ===\n",
        "bins = np.arange(0, 1.01, 0.1)\n",
        "bin_labels = [f'{b:.1f}-{b + 0.1:.1f}' for b in bins[:-1]]\n",
        "df_filtered_ce_deberta['confidence_bin'] = pd.cut(df_filtered_ce_deberta['confidence'], bins=bins, right=False, labels=bin_labels)\n",
        "\n",
        "# === Accuracy per bin for Cross-Encoder DeBERTa ===\n",
        "confidence_accuracy_ce_deberta = df_filtered_ce_deberta.groupby('confidence_bin', observed=True)['is_correct'].mean().reset_index()\n",
        "confidence_accuracy_ce_deberta = confidence_accuracy_ce_deberta.rename(columns={'is_correct': 'accuracy'})\n",
        "\n",
        "# === Count per bin for Cross-Encoder DeBERTa ===\n",
        "bin_counts_ce_deberta = df_filtered_ce_deberta.groupby('confidence_bin', observed=True).size().reset_index(name='count')\n",
        "\n",
        "# === Merge accuracy + counts for Cross-Encoder DeBERTa ===\n",
        "confidence_data_ce_deberta = pd.merge(confidence_accuracy_ce_deberta, bin_counts_ce_deberta, on='confidence_bin')\n",
        "\n",
        "# === Cumulative calculations for Cross-Encoder DeBERTa ===\n",
        "confidence_data_ce_deberta['bin_midpoint'] = confidence_data_ce_deberta['confidence_bin'].apply(\n",
        "    lambda x: float(x.split('-')[0]) + 0.05 if pd.notna(x) else np.nan\n",
        ")\n",
        "confidence_data_ce_deberta = confidence_data_ce_deberta.sort_values('bin_midpoint').reset_index(drop=True)\n",
        "confidence_data_ce_deberta['cumulative_count'] = confidence_data_ce_deberta['count'].cumsum()\n",
        "confidence_data_ce_deberta['cumulative_correct'] = (confidence_data_ce_deberta['accuracy'] * confidence_data_ce_deberta['count']).cumsum()\n",
        "confidence_data_ce_deberta['cumulative_accuracy'] = confidence_data_ce_deberta['cumulative_correct'] / confidence_data_ce_deberta['cumulative_count']\n",
        "\n",
        "# === Clean up and drop bins with no accuracy for Cross-Encoder DeBERTa ===\n",
        "confidence_data_ce_deberta = confidence_data_ce_deberta.drop(columns=['bin_midpoint'])\n",
        "confidence_data_ce_deberta = confidence_data_ce_deberta.dropna(subset=['cumulative_accuracy'])\n",
        "\n",
        "# === Plot Confidence Curve for Cross-Encoder DeBERTa ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(confidence_data_ce_deberta['confidence_bin'], confidence_data_ce_deberta['cumulative_accuracy'],\n",
        "         marker='o', linestyle='-', label='Cumulative Accuracy (Cross-Encoder DeBERTa)')\n",
        "\n",
        "# Horizontal line for overall accuracy for Cross-Encoder DeBERTa\n",
        "overall_accuracy_ce_deberta = df_filtered_ce_deberta['is_correct'].mean()\n",
        "plt.axhline(y=overall_accuracy_ce_deberta, color='brown', linestyle='--', label='Overall Accuracy (Cross-Encoder DeBERTa)')\n",
        "\n",
        "# Vertical line for threshold for Cross-Encoder DeBERTa\n",
        "threshold_bin_ce_deberta = f'{THRESHOLD_CE_DEBERTA:.1f}-{THRESHOLD_CE_DEBERTA + 0.1:.1f}'\n",
        "if threshold_bin_ce_deberta in confidence_data_ce_deberta['confidence_bin'].values:\n",
        "    plt.axvline(x=threshold_bin_ce_deberta, color='cyan', linestyle='--', label=f'Threshold ({THRESHOLD_CE_DEBERTA:.1f})')\n",
        "\n",
        "# === Plot formatting ===\n",
        "plt.xlabel('Confidence Bin')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Confidence Curve (cross-encoder/nli-deberta-v3-base)')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confidence_curve_ce_deberta.png')\n",
        "plt.show()\n",
        "\n",
        "# === Print table ===\n",
        "print(\"\\nConfidence Data (Cross-Encoder DeBERTa):\")\n",
        "print(confidence_data_ce_deberta[['confidence_bin', 'count', 'accuracy', 'cumulative_count', 'cumulative_accuracy']])\n",
        "\n",
        "# Plotting the distribution of confidence scores for the Cross-Encoder DeBERTa model\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_results_ce_deberta['confidence'], bins=20, kde=True)\n",
        "plt.title('Distribution of Confidence Scores (Cross-Encoder DeBERTa)')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.savefig('confidence_score_distribution_ce_deberta.png')\n",
        "plt.show()\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results_ce_deberta = pd.DataFrame(saved_results_ce_deberta)\n",
        "\n",
        "# Map predicted labels to 'real', 'fake', or 'uncertain'\n",
        "label_mapping = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\": \"fake\"\n",
        "}\n",
        "df_results_ce_deberta['predicted_label_mapped'] = df_results_ce_deberta['predicted_label'].map(label_mapping).fillna('uncertain')\n",
        "\n",
        "# Filter out 'uncertain' predictions\n",
        "df_filtered_ce_deberta = df_results_ce_deberta[df_results_ce_deberta['predicted_label_mapped'] != 'uncertain'].copy()\n",
        "df_filtered_ce_deberta['is_correct'] = (df_filtered_ce_deberta['true_label'] == df_filtered_ce_deberta['predicted_label_mapped'])\n",
        "\n",
        "# Get true and predicted labels from the filtered DataFrame for Cross-Encoder DeBERTa\n",
        "true_labels_filtered_ce_deberta = df_filtered_ce_deberta['true_label']\n",
        "predicted_labels_filtered_ce_deberta = df_filtered_ce_deberta['predicted_label_mapped']\n",
        "\n",
        "# Generate the confusion matrix for Cross-Encoder DeBERTa\n",
        "cm_ce_deberta = confusion_matrix(true_labels_filtered_ce_deberta, predicted_labels_filtered_ce_deberta, labels=['real', 'fake'])\n",
        "\n",
        "# Plot the confusion matrix for Cross-Encoder DeBERTa\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_ce_deberta, annot=True, fmt='d', cmap='Blues', xticklabels=['real', 'fake'], yticklabels=['real', 'fake'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Cross-Encoder DeBERTa - Filtered: Excluding \"uncertain\")')\n",
        "plt.savefig('confusion_matrix_filtered_ce_deberta.png')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Calculate and print confusion matrix values for Cross-Encoder DeBERTa\n",
        "print(\"\\nConfusion Matrix (Cross-Encoder DeBERTa):\")\n",
        "cm_ce_deberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3XaQjUT0eii"
      },
      "outputs": [],
      "source": [
        "# === Load saved results ===\n",
        "results_path = \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results(0.8).json\"\n",
        "\n",
        "with open(results_path, \"r\") as f:\n",
        "    saved_results = json.load(f)\n",
        "\n",
        "# === Define candidate labels (must match what was used during inference) ===\n",
        "LABELS = [\n",
        "    \"This news is real and based on facts.\",\n",
        "    \"This news is fake and contains misinformation.\"\n",
        "]\n",
        "\n",
        "# === Filter out uncertain predictions ===\n",
        "filtered_results = [\n",
        "    result for result in saved_results if result[\"predicted_label\"] != \"uncertain\"\n",
        "]\n",
        "\n",
        "# === Check and evaluate ===\n",
        "if len(filtered_results) == 0:\n",
        "    print(\"‚ùå No decided predictions (above threshold) to evaluate.\")\n",
        "else:\n",
        "    # Extract ground-truth and predicted labels\n",
        "    true_labels_eval = [result[\"true_label\"] for result in filtered_results]\n",
        "    predicted_labels_eval = [\n",
        "        'real' if result[\"predicted_label\"] == LABELS[0] else 'fake'\n",
        "        for result in filtered_results\n",
        "    ]\n",
        "\n",
        "    # Define fixed label order\n",
        "    labels = ['real', 'fake']\n",
        "\n",
        "    # Compute and show confusion matrix\n",
        "    cm = confusion_matrix(true_labels_eval, predicted_labels_eval, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix - Cross-Encoder DeBERTa (Threshold 0.8, Excluding Uncertain)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = accuracy_score(true_labels_eval, predicted_labels_eval)\n",
        "    print(f\"‚úÖ Accuracy (excluding uncertain): {accuracy:.4f}\")\n",
        "\n",
        "# === Report counts ===\n",
        "num_uncertain = len([\n",
        "    result for result in saved_results if result[\"predicted_label\"] == \"uncertain\"\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H6a2ZzecB9Q"
      },
      "source": [
        "## Evaluation and compare before ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhiTQSupz3nx"
      },
      "outputs": [],
      "source": [
        "!pip install kaleido==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q73k4yTlcI1M"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# === Load and Calculate Metrics ===\n",
        "def calculate_metrics(results_path, threshold=0.8, labels=[\"This news is real and based on facts.\", \"This news is fake and contains misinformation.\"]):\n",
        "    with open(results_path, \"r\") as f:\n",
        "        saved_results = json.load(f)\n",
        "\n",
        "    filtered_results = [r for r in saved_results if r[\"predicted_label\"] != \"uncertain\"]\n",
        "    num_uncertain = len(saved_results) - len(filtered_results)\n",
        "    total_predictions = len(saved_results)\n",
        "\n",
        "    if not filtered_results:\n",
        "        return None, None, None, None, None, num_uncertain, total_predictions\n",
        "\n",
        "    true_labels_eval = [r[\"true_label\"] for r in filtered_results]\n",
        "    label_map = {labels[0]: 'real', labels[1]: 'fake'}\n",
        "    predicted_labels_eval = [label_map.get(r[\"predicted_label\"], 'uncertain') for r in filtered_results]\n",
        "\n",
        "    # Filter again if any 'uncertain' slipped through\n",
        "    true_labels_eval = [t for i, t in enumerate(true_labels_eval) if predicted_labels_eval[i] in ['real', 'fake']]\n",
        "    predicted_labels_eval = [p for p in predicted_labels_eval if p in ['real', 'fake']]\n",
        "\n",
        "    if not true_labels_eval or len(true_labels_eval) != len(predicted_labels_eval):\n",
        "        return None, None, None, None, None, num_uncertain, total_predictions\n",
        "\n",
        "    accuracy = accuracy_score(true_labels_eval, predicted_labels_eval)\n",
        "    precision = precision_score(true_labels_eval, predicted_labels_eval, average='weighted', zero_division=0)\n",
        "    recall = recall_score(true_labels_eval, predicted_labels_eval, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(true_labels_eval, predicted_labels_eval, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1, None, num_uncertain, total_predictions\n",
        "\n",
        "\n",
        "# === Model Paths ===\n",
        "paths = {\n",
        "    \"DeBERTa\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Deberta_maxscore_results(0.8).json\",\n",
        "    \"BART\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/bart_maxscore_results(0.8).json\",\n",
        "    \"ModernBERT\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Modernbert_maxscore_results(0.8).json\",\n",
        "    \"CE-DeBERTa\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results(0.8).json\"\n",
        "}\n",
        "\n",
        "metrics = {}\n",
        "\n",
        "for model_name, path in paths.items():\n",
        "    acc, prec, rec, f1_sc, _, uncertain, total = calculate_metrics(path)\n",
        "    if acc is not None:\n",
        "        metrics[model_name] = {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision\": prec,\n",
        "            \"Recall\": rec,\n",
        "            \"F1-Score\": f1_sc,\n",
        "            \"Uncertain\": uncertain / total if total > 0 else 0\n",
        "        }\n",
        "\n",
        "# === Convert to DataFrame ===\n",
        "metrics_df = pd.DataFrame(metrics).T\n",
        "print(\"\\nüîç Metrics Table:\\n\", metrics_df)\n",
        "\n",
        "# === 1. Combined Bar Chart ===\n",
        "metrics_df[['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Uncertain']].plot(\n",
        "    kind='bar',\n",
        "    figsize=(12, 7),\n",
        "    colormap='viridis'\n",
        ")\n",
        "plt.title('Model Performance Comparison (Including Uncertainty)', fontsize=14)\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_performance_bar_chart_with_uncertainty.png')\n",
        "plt.show()\n",
        "\n",
        "# === 2. Individual Bar Charts ===\n",
        "fig, axes = plt.subplots(3, 2, figsize=(14, 15))\n",
        "axes = axes.flatten()\n",
        "metrics_list = ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Uncertain']\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'orchid']\n",
        "\n",
        "for i, metric in enumerate(metrics_list):\n",
        "    metrics_df[metric].plot(kind='bar', ax=axes[i], color=colors[i])\n",
        "    axes[i].set_title(f'{metric} Comparison')\n",
        "    axes[i].set_ylabel(metric)\n",
        "    axes[i].set_ylim(0, 1.1)\n",
        "    axes[i].tick_params(axis='x', rotation=0)\n",
        "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Hide empty 6th subplot\n",
        "if len(axes) > len(metrics_list):\n",
        "    for j in range(len(metrics_list), len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_individual_metrics_bar_charts_with_uncertainty.png')\n",
        "plt.show()\n",
        "\n",
        "# === 3. Radar Chart ===\n",
        "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Uncertain']\n",
        "fig = go.Figure()\n",
        "desired_order = [\"CE-DeBERTa\", \"DeBERTa\", \"BART\", \"ModernBERT\"]\n",
        "\n",
        "for model in desired_order:\n",
        "    values = [metrics[model].get(c, 0) for c in categories]\n",
        "    values += values[:1]\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=values,\n",
        "        theta=categories + categories[:1],\n",
        "        fill='toself',\n",
        "        name=model\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
        "    showlegend=True,\n",
        "    title='Model Performance Radar Chart (Including Uncertainty)',\n",
        "    height=700,\n",
        "    width=900\n",
        ")\n",
        "fig.write_image(\"model_performance_radar_chart_with_uncertainty.png\")  # requires kaleido\n",
        "fig.show()\n",
        "\n",
        "# === 4. Heatmap ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(metrics_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Uncertain']].T,\n",
        "            annot=True, fmt=\".3f\", cmap=\"PuBuGn\")  # dreamy blue-green\n",
        "plt.title('Model Performance Heatmap (Including Uncertainty)', fontsize=14)\n",
        "plt.ylabel('Metric')\n",
        "plt.xlabel('Model')\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_performance_heatmap_with_uncertainty.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilpXG7zodc7Y"
      },
      "outputs": [],
      "source": [
        "# === Add Certainty and Performance Score ===\n",
        "metrics_df['Certainty'] = 1 - metrics_df['Uncertain']\n",
        "metrics_df['Performance_Score'] = metrics_df[\n",
        "    ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Certainty']\n",
        "].mean(axis=1)\n",
        "\n",
        "# Optional: Print table\n",
        "print(\"\\nüìä Model Performance Scores:\\n\", metrics_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Uncertain', 'Certainty', 'Performance_Score']])\n",
        "# === 5. Performance Score Bar Chart ===\n",
        "plt.figure(figsize=(8, 6))\n",
        "metrics_df['Performance_Score'].plot(\n",
        "    kind='bar',\n",
        "    color='mediumseagreen',\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Performance Score of Zero-Shot Models', fontsize=14)\n",
        "plt.ylabel('Performance Score')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_performance_score_bar_chart.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01LlOEYs1O1i"
      },
      "source": [
        "## Applying Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDdxbncc1R_j"
      },
      "outputs": [],
      "source": [
        "# === Paths to model outputs ===\n",
        "paths = {\n",
        "    \"modernbert\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Modernbert_maxscore_results(0.8).json\",\n",
        "    \"bart\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/bart_maxscore_results(0.8).json\",\n",
        "    \"deberta\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Deberta_maxscore_results(0.8).json\",\n",
        "    \"crossencoder\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results1.json\"\n",
        "}\n",
        "\n",
        "# === Load model predictions ===\n",
        "all_results = {}\n",
        "for model_name, path in paths.items():\n",
        "    with open(path, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "        all_results[model_name] = {r[\"index\"]: r for r in results}\n",
        "\n",
        "# === Perform majority vote ===\n",
        "ensemble_results = []\n",
        "for idx in all_results[\"modernbert\"]:\n",
        "    votes = []\n",
        "    true_label = all_results[\"modernbert\"][idx][\"true_label\"]\n",
        "\n",
        "    for model in all_results:\n",
        "        label = all_results[model][idx][\"predicted_label\"]\n",
        "        if \"real\" in label.lower():\n",
        "            votes.append(\"real\")\n",
        "        elif \"fake\" in label.lower():\n",
        "            votes.append(\"fake\")\n",
        "        else:\n",
        "            votes.append(\"uncertain\")\n",
        "\n",
        "    # Remove uncertain votes\n",
        "    decided_votes = [v for v in votes if v != \"uncertain\"]\n",
        "\n",
        "    if decided_votes:\n",
        "        final_vote = Counter(decided_votes).most_common(1)[0][0]\n",
        "        ensemble_results.append({\n",
        "            \"index\": idx,\n",
        "            \"true_label\": true_label,\n",
        "            \"votes\": votes,\n",
        "            \"final_prediction\": final_vote\n",
        "        })\n",
        "\n",
        "# === Evaluate ensemble ===\n",
        "true_labels = [r[\"true_label\"] for r in ensemble_results]\n",
        "predicted_labels = [r[\"final_prediction\"] for r in ensemble_results]\n",
        "\n",
        "# Confusion Matrix\n",
        "labels = [\"real\", \"fake\"]\n",
        "cm = confusion_matrix(true_labels, predicted_labels, labels=labels)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Ensemble (Majority Voting)\")\n",
        "plt.savefig('confusion_matrix_ensemble.png')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"‚úÖ Accuracy (excluding uncertain): {accuracy:.4f}\")\n",
        "print(f\"Total evaluated predictions: {len(predicted_labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqWOYDyOCqMj"
      },
      "outputs": [],
      "source": [
        "# === Paths to model outputs ===\n",
        "paths = {\n",
        "    \"modernbert\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Modernbert_maxscore_results(0.8).json\",\n",
        "    \"bart\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/bart_maxscore_results(0.8).json\",\n",
        "    \"deberta\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Deberta_maxscore_results(0.8).json\",\n",
        "    \"crossencoder\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results(0.8).json\"\n",
        "}\n",
        "\n",
        "THRESHOLD = 0.85  # Minimum soft score for decision\n",
        "\n",
        "# === Load model predictions ===\n",
        "all_results = {}\n",
        "for model_name, path in paths.items():\n",
        "    with open(path, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "        all_results[model_name] = {r[\"index\"]: r for r in results}\n",
        "\n",
        "# === Soft voting ensemble ===\n",
        "ensemble_results = []\n",
        "\n",
        "for idx in all_results[\"modernbert\"]:  # assume all models share same indexes\n",
        "    true_label = all_results[\"modernbert\"][idx][\"true_label\"]\n",
        "\n",
        "    score_real = 0.0\n",
        "    score_fake = 0.0\n",
        "    confidence_count = 0\n",
        "\n",
        "    for model in all_results:\n",
        "        result = all_results[model][idx]\n",
        "        label = result[\"predicted_label\"].lower()\n",
        "        confidence = result[\"confidence\"]\n",
        "\n",
        "        if \"real\" in label:\n",
        "            score_real += confidence\n",
        "            confidence_count += 1\n",
        "        elif \"fake\" in label:\n",
        "            score_fake += confidence\n",
        "            confidence_count += 1\n",
        "        # \"uncertain\" labels are ignored\n",
        "\n",
        "    # Final decision based on max soft score if above threshold\n",
        "    if max(score_real, score_fake) >= THRESHOLD:\n",
        "        final_prediction = \"real\" if score_real > score_fake else \"fake\"\n",
        "    else:\n",
        "        final_prediction = \"uncertain\"\n",
        "\n",
        "    ensemble_results.append({\n",
        "        \"index\": idx,\n",
        "        \"true_label\": true_label,\n",
        "        \"score_real\": round(score_real, 4),\n",
        "        \"score_fake\": round(score_fake, 4),\n",
        "        \"final_prediction\": final_prediction\n",
        "    })\n",
        "\n",
        "# === Evaluate ===\n",
        "filtered = [r for r in ensemble_results if r[\"final_prediction\"] != \"uncertain\"]\n",
        "true_labels = [r[\"true_label\"] for r in filtered]\n",
        "predicted_labels = [r[\"final_prediction\"] for r in filtered]\n",
        "\n",
        "labels = [\"real\", \"fake\"]\n",
        "cm = confusion_matrix(true_labels, predicted_labels, labels=labels)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(f\"Confusion Matrix - Soft Voting Ensemble (Threshold={THRESHOLD})\")\n",
        "plt.savefig('confusion_matrix_soft_voting.png')\n",
        "plt.show()\n",
        "\n",
        "# === Accuracy ===\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"‚úÖ Accuracy (excluding uncertain): {accuracy:.4f}\")\n",
        "print(f\"Total evaluated predictions: {len(predicted_labels)}\")\n",
        "print(f\"Total uncertain predictions: {len(ensemble_results) - len(predicted_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZADklUmFDqD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# === Paths to model outputs ===\n",
        "paths = {\n",
        "    \"modernbert\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Modernbert_maxscore_results(0.8).json\",\n",
        "    \"bart\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/bart_maxscore_results(0.8).json\",\n",
        "    \"deberta\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Deberta_maxscore_results(0.8).json\",\n",
        "    \"crossencoder\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results(0.8).json\"\n",
        "}\n",
        "\n",
        "THRESHOLD = 0.92  # Confidence threshold for the final decision\n",
        "\n",
        "# === Load all model results ===\n",
        "all_results = {}\n",
        "for model_name, path in paths.items():\n",
        "    with open(path, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "        all_results[model_name] = {r[\"index\"]: r for r in results}\n",
        "\n",
        "# === Soft voting ensemble with confidence threshold ===\n",
        "ensemble_results = []\n",
        "\n",
        "for idx in all_results[\"modernbert\"]:  # All share same indices\n",
        "    true_label = all_results[\"modernbert\"][idx][\"true_label\"]\n",
        "\n",
        "    score_real = 0.0\n",
        "    score_fake = 0.0\n",
        "\n",
        "    for model in all_results:\n",
        "        result = all_results[model][idx]\n",
        "        label = result[\"predicted_label\"].lower()\n",
        "        confidence = result[\"confidence\"]\n",
        "\n",
        "        if \"real\" in label:\n",
        "            score_real += confidence\n",
        "        elif \"fake\" in label:\n",
        "            score_fake += confidence\n",
        "        # ignore 'uncertain'\n",
        "\n",
        "    # Apply final thresholding\n",
        "    if max(score_real, score_fake) >= THRESHOLD:\n",
        "        final_prediction = \"real\" if score_real > score_fake else \"fake\"\n",
        "    else:\n",
        "        final_prediction = \"uncertain\"\n",
        "\n",
        "    ensemble_results.append({\n",
        "        \"index\": idx,\n",
        "        \"true_label\": true_label,\n",
        "        \"score_real\": round(score_real, 4),\n",
        "        \"score_fake\": round(score_fake, 4),\n",
        "        \"final_prediction\": final_prediction\n",
        "    })\n",
        "\n",
        "# === Evaluation ===\n",
        "filtered = [r for r in ensemble_results if r[\"final_prediction\"] != \"uncertain\"]\n",
        "true_labels = [r[\"true_label\"] for r in filtered]\n",
        "predicted_labels = [r[\"final_prediction\"] for r in filtered]\n",
        "\n",
        "labels = [\"real\", \"fake\"]\n",
        "cm = confusion_matrix(true_labels, predicted_labels, labels=labels)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(f\"Confusion Matrix - Soft Voting Ensemble (Threshold={THRESHOLD})\")\n",
        "plt.savefig('confusion_matrix_soft_voting(.92).png')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"‚úÖ Accuracy (excluding uncertain): {accuracy:.4f}\")\n",
        "print(f\"üßÆ Total evaluated predictions: {len(predicted_labels)}\")\n",
        "print(f\"üï≥Ô∏è Total uncertain predictions: {len(ensemble_results) - len(predicted_labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKzFfJhM2JBe"
      },
      "outputs": [],
      "source": [
        "# prompt: # --- Ensemble Metrics Visualization with Uncertainty ---\n",
        "\n",
        "# === Paths to model outputs ===\n",
        "paths = {\n",
        "    \"modernbert\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Modernbert_maxscore_results(0.8).json\",\n",
        "    \"bart\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/bart_maxscore_results(0.8).json\",\n",
        "    \"deberta\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/Deberta_maxscore_results(0.8).json\",\n",
        "    \"crossencoder\": \"/content/drive/MyDrive/DatasetForColab/Zeroshot1/ce_deberta_maxscore_results(0.8).json\"\n",
        "}\n",
        "\n",
        "LABELS_MAP = {\n",
        "    \"This news is real and based on facts.\": \"real\",\n",
        "    \"This news is fake and contains misinformation.\": \"fake\"\n",
        "}\n",
        "\n",
        "# === Load and process results for all models ===\n",
        "all_model_data = {}\n",
        "for model_name, path in paths.items():\n",
        "    with open(path, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "    all_model_data[model_name] = {\n",
        "        r[\"index\"]: {\n",
        "            \"true_label\": r[\"true_label\"],\n",
        "            \"predicted_label\": LABELS_MAP.get(r[\"predicted_label\"], \"uncertain\"),\n",
        "            \"confidence\": r[\"confidence\"]\n",
        "        }\n",
        "        for r in results\n",
        "    }\n",
        "\n",
        "# Get the set of all unique indices across all models\n",
        "all_indices = sorted(list(set().union(*[data.keys() for data in all_model_data.values()])))\n",
        "\n",
        "# === Calculate metrics for each ensemble method ===\n",
        "\n",
        "def evaluate_ensemble(ensemble_results):\n",
        "    \"\"\"Evaluates the ensemble results.\"\"\"\n",
        "    filtered_results = [r for r in ensemble_results if r[\"final_prediction\"] != \"uncertain\"]\n",
        "    num_uncertain = len(ensemble_results) - len(filtered_results)\n",
        "    total_predictions = len(ensemble_results)\n",
        "\n",
        "    if not filtered_results:\n",
        "        return {\n",
        "            \"Accuracy\": 0.0,\n",
        "            \"Precision\": 0.0,\n",
        "            \"Recall\": 0.0,\n",
        "            \"F1-Score\": 0.0,\n",
        "            \"Uncertain\": num_uncertain / total_predictions if total_predictions > 0 else 0.0,\n",
        "            \"Evaluated_Count\": 0,\n",
        "            \"Total_Count\": total_predictions\n",
        "        }\n",
        "\n",
        "    true_labels_eval = [r[\"true_label\"] for r in filtered_results]\n",
        "    predicted_labels_eval = [r[\"final_prediction\"] for r in filtered_results]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels_eval, predicted_labels_eval)\n",
        "    precision = precision_score(true_labels_eval, predicted_labels_eval, average='weighted', zero_division=0)\n",
        "    recall = recall_score(true_labels_eval, predicted_labels_eval, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(true_labels_eval, predicted_labels_eval, average='weighted', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"Uncertain\": num_uncertain / total_predictions if total_predictions > 0 else 0.0,\n",
        "        \"Evaluated_Count\": len(filtered_results),\n",
        "        \"Total_Count\": total_predictions\n",
        "    }\n",
        "\n",
        "# --- Majority Voting Ensemble ---\n",
        "majority_vote_results = []\n",
        "for idx in all_indices:\n",
        "    votes = []\n",
        "    true_label = None\n",
        "    for model_name in all_model_data:\n",
        "        if idx in all_model_data[model_name]:\n",
        "            votes.append(all_model_data[model_name][idx][\"predicted_label\"])\n",
        "            if true_label is None:\n",
        "                 true_label = all_model_data[model_name][idx][\"true_label\"]\n",
        "\n",
        "\n",
        "    # Remove uncertain votes\n",
        "    decided_votes = [v for v in votes if v != \"uncertain\"]\n",
        "\n",
        "    final_prediction = \"uncertain\"\n",
        "    if decided_votes:\n",
        "         # Handle cases where a model might be missing for an index\n",
        "        if true_label is None:\n",
        "           # Find true label from the first model that has this index\n",
        "           for model_name in all_model_data:\n",
        "               if idx in all_model_data[model_name]:\n",
        "                   true_label = all_model_data[model_name][idx][\"true_label\"]\n",
        "                   break\n",
        "\n",
        "        vote_counts = Counter(decided_votes)\n",
        "        max_count = max(vote_counts.values())\n",
        "        most_common = [label for label, count in vote_counts.items() if count == max_count]\n",
        "\n",
        "        if len(most_common) == 1: # Clear majority\n",
        "             final_prediction = most_common[0]\n",
        "        else: # Tie, mark as uncertain\n",
        "            final_prediction = \"uncertain\"\n",
        "\n",
        "\n",
        "    majority_vote_results.append({\n",
        "        \"index\": idx,\n",
        "        \"true_label\": true_label,\n",
        "        \"votes\": votes, # Optional: Keep votes for inspection\n",
        "        \"final_prediction\": final_prediction\n",
        "    })\n",
        "\n",
        "majority_vote_metrics = evaluate_ensemble(majority_vote_results)\n",
        "\n",
        "\n",
        "# --- Soft Voting Ensemble with Threshold 0.85 ---\n",
        "soft_vote_085_results = []\n",
        "threshold_085 = 0.85\n",
        "for idx in all_indices:\n",
        "    true_label = None\n",
        "    score_real = 0.0\n",
        "    score_fake = 0.0\n",
        "    model_count_for_index = 0\n",
        "\n",
        "    for model_name in all_model_data:\n",
        "        if idx in all_model_data[model_name]:\n",
        "            result = all_model_data[model_name][idx]\n",
        "            if true_label is None:\n",
        "                true_label = result[\"true_label\"]\n",
        "\n",
        "            label = result[\"predicted_label\"]\n",
        "            confidence = result[\"confidence\"]\n",
        "            model_count_for_index += 1 # Count models that actually have a prediction for this index\n",
        "\n",
        "            if label == \"real\":\n",
        "                score_real += confidence\n",
        "            elif label == \"fake\":\n",
        "                score_fake += confidence\n",
        "\n",
        "    # Normalize scores if needed, or use sum directly with threshold\n",
        "    # Using sum directly with threshold as implemented previously\n",
        "    if max(score_real, score_fake) >= threshold_085:\n",
        "        final_prediction = \"real\" if score_real > score_fake else \"fake\"\n",
        "    else:\n",
        "        final_prediction = \"uncertain\"\n",
        "\n",
        "    soft_vote_085_results.append({\n",
        "        \"index\": idx,\n",
        "        \"true_label\": true_label,\n",
        "        \"score_real\": round(score_real, 4),\n",
        "        \"score_fake\": round(score_fake, 4),\n",
        "        \"final_prediction\": final_prediction\n",
        "    })\n",
        "\n",
        "soft_vote_085_metrics = evaluate_ensemble(soft_vote_085_results)\n",
        "\n",
        "\n",
        "# --- Soft Voting Ensemble with Threshold 0.92 ---\n",
        "soft_vote_092_results = []\n",
        "threshold_092 = 0.92\n",
        "for idx in all_indices:\n",
        "    true_label = None\n",
        "    score_real = 0.0\n",
        "    score_fake = 0.0\n",
        "    model_count_for_index = 0\n",
        "\n",
        "    for model_name in all_model_data:\n",
        "        if idx in all_model_data[model_name]:\n",
        "            result = all_model_data[model_name][idx]\n",
        "            if true_label is None:\n",
        "                true_label = result[\"true_label\"]\n",
        "\n",
        "            label = result[\"predicted_label\"]\n",
        "            confidence = result[\"confidence\"]\n",
        "            model_count_for_index += 1\n",
        "\n",
        "            if label == \"real\":\n",
        "                score_real += confidence\n",
        "            elif label == \"fake\":\n",
        "                score_fake += confidence\n",
        "\n",
        "    if max(score_real, score_fake) >= threshold_092:\n",
        "        final_prediction = \"real\" if score_real > score_fake else \"fake\"\n",
        "    else:\n",
        "        final_prediction = \"uncertain\"\n",
        "\n",
        "    soft_vote_092_results.append({\n",
        "        \"index\": idx,\n",
        "        \"true_label\": true_label,\n",
        "        \"score_real\": round(score_real, 4),\n",
        "        \"score_fake\": round(score_fake, 4),\n",
        "        \"final_prediction\": final_prediction\n",
        "    })\n",
        "\n",
        "soft_vote_092_metrics = evaluate_ensemble(soft_vote_092_results)\n",
        "\n",
        "\n",
        "# --- Collect all ensemble metrics ---\n",
        "ensemble_metrics = {\n",
        "    \"Majority Vote\": majority_vote_metrics,\n",
        "    \"Soft Vote (Thresh=0.85)\": soft_vote_085_metrics,\n",
        "    \"Soft Vote (Thresh=0.92)\": soft_vote_092_metrics\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "ensemble_metrics_df = pd.DataFrame(ensemble_metrics).T\n",
        "\n",
        "# Add Certainty and Performance Score\n",
        "ensemble_metrics_df['Certainty'] = 1 - ensemble_metrics_df['Uncertain']\n",
        "ensemble_metrics_df['Performance_Score'] = ensemble_metrics_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Certainty']].mean(axis=1)\n",
        "\n",
        "# === Save Metrics Table ===\n",
        "ensemble_metrics_df_rounded = ensemble_metrics_df.round(4)\n",
        "ensemble_metrics_df_rounded.to_csv(\"ensemble_metrics_with_uncertainty.csv\")\n",
        "print(\"\\nEnsemble Performance Metrics:\")\n",
        "print(ensemble_metrics_df_rounded)\n",
        "\n",
        "# === 1. Combined Bar Chart ===\n",
        "metrics_for_bar = ensemble_metrics_df[['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Uncertain']]\n",
        "metrics_for_bar.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n",
        "plt.title('Ensemble Performance Comparison (Including Uncertainty)')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig('ensemble_performance_bar_chart_with_uncertainty.png')\n",
        "plt.show()\n",
        "\n",
        "# === 2. Individual Metric Bar Charts ===\n",
        "fig, axes = plt.subplots(3, 2, figsize=(14, 15))\n",
        "axes = axes.flatten()\n",
        "metrics_list = ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Uncertain']\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum']\n",
        "\n",
        "for i, metric in enumerate(metrics_list):\n",
        "    ensemble_metrics_df[metric].plot(kind='bar', ax=axes[i], color=colors[i % len(colors)])\n",
        "    axes[i].set_title(f'{metric} Comparison')\n",
        "    axes[i].set_ylabel(metric)\n",
        "    axes[i].set_ylim(0, 1.1)\n",
        "    axes[i].tick_params(axis='x', rotation=0)\n",
        "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Hide unused subplot\n",
        "if len(axes) > len(metrics_list):\n",
        "    for j in range(len(metrics_list), len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ensemble_individual_metrics_bar_charts_with_uncertainty.png')\n",
        "plt.show()\n",
        "\n",
        "# === 3. Heatmap ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(ensemble_metrics_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Uncertain']].T,\n",
        "            annot=True, fmt=\".3f\", cmap=\"coolwarm\")\n",
        "plt.title('Ensemble Performance Heatmap (Including Uncertainty)')\n",
        "plt.ylabel('Metric')\n",
        "plt.xlabel('Ensemble Method')\n",
        "plt.tight_layout()\n",
        "plt.savefig('ensemble_performance_heatmap_with_uncertainty.png')\n",
        "plt.show()\n",
        "\n",
        "# === 4. Radar Chart ===\n",
        "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Uncertain']\n",
        "fig_radar = go.Figure()\n",
        "\n",
        "# Sort by overall performance score (lowest to highest)\n",
        "desired_order = ensemble_metrics_df.sort_values('Performance_Score').index.tolist()\n",
        "\n",
        "for ensemble_name in desired_order:\n",
        "    values = [ensemble_metrics_df.loc[ensemble_name][cat] for cat in categories]\n",
        "    values += values[:1]  # Close the loop\n",
        "    fig_radar.add_trace(go.Scatterpolar(\n",
        "        r=values,\n",
        "        theta=categories + categories[:1],\n",
        "        fill='toself',\n",
        "        name=ensemble_name\n",
        "    ))\n",
        "\n",
        "fig_radar.update_layout(\n",
        "    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
        "    showlegend=True,\n",
        "    title='Ensemble Performance Radar Chart (Including Uncertainty)',\n",
        "    height=700,\n",
        "    width=900\n",
        ")\n",
        "\n",
        "# Save Radar Chart (requires compatible plotly and kaleido versions)\n",
        "try:\n",
        "    fig_radar.write_image(\"ensemble_performance_radar_chart_with_uncertainty.png\")\n",
        "except ValueError as e:\n",
        "    print(\"\\n‚ö†Ô∏è Radar chart image export failed. Please update plotly and/or kaleido:\")\n",
        "    print(\" - pip install -U plotly\")\n",
        "    print(\" - or pip install kaleido==0.2.1\\n\")\n",
        "\n",
        "fig_radar.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iORrYwxddKaH"
      },
      "outputs": [],
      "source": [
        "# === Plot Performance Score for Ensemble Methods ===\n",
        "plt.figure(figsize=(8, 6))\n",
        "ensemble_metrics_df['Performance_Score'].plot(\n",
        "    kind='bar',\n",
        "    color='mediumseagreen',\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Performance Score of Ensemble Methods', fontsize=14)\n",
        "plt.ylabel('Performance Score', fontsize=12)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig('ensemble_performance_score_bar_chart.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nbformat\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive if your notebook is saved there\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Replace with your actual notebook path\n",
        "# notebook_path = '/content/drive/MyDrive/Colab Notebooks/Fake_News_Detection.ipynb'\n"
      ],
      "metadata": {
        "id": "GfRnB-Z04s0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# path = '/content/drive/MyDrive/Colab Notebooks/Fake_News_Detection.ipynb'\n",
        "# print(\"Exists:\", os.path.exists(path))\n"
      ],
      "metadata": {
        "id": "26u_gBN742Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nbformat\n",
        "\n",
        "# # Path to your notebook\n",
        "# notebook_path = '/content/drive/MyDrive/Colab Notebooks/Fake_News_Detection.ipynb'\n",
        "\n",
        "# # Load notebook\n",
        "# with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "#     nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "# # Remove corrupted widget metadata\n",
        "# if 'widgets' in nb.metadata:\n",
        "#     del nb.metadata['widgets']\n",
        "\n",
        "# # Rebuild clean widget metadata\n",
        "# nb.metadata['widgets'] = {\n",
        "#     \"state\": {},\n",
        "#     \"application/vnd.jupyter.widget-state+json\": {\n",
        "#         \"version_major\": 2,\n",
        "#         \"version_minor\": 0,\n",
        "#         \"state\": {}\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # Save notebook\n",
        "# with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "#     nbformat.write(nb, f)\n",
        "\n",
        "# print(\"‚úÖ Widget metadata rebuilt. Now re-run your notebook cells to reinitialize widgets.\")\n"
      ],
      "metadata": {
        "id": "XkJRE7zx4o_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Sa_xunM5g0E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X8-F70Zu5EGC",
        "5balq7DO5Y8B",
        "54167-_c76YH",
        "p9WGZM-gA2mF",
        "LdyD92KY8NM3",
        "QjS_8kyu5dVt",
        "LN3z1iCI8E6A",
        "X8EomFI09G0g",
        "23c1CWO4zcVy",
        "8ExHDnJB0Bg6",
        "CYk6p8Kc0iEj",
        "3H6a2ZzecB9Q",
        "01LlOEYs1O1i"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}